# this file is used to configure the training of the DQN agent for MILP comparison
project: Multi_BESS_hyperparameter_tuning
program: scripts/train_test_dqn_da_id_wandb.py
method: random  # random, grid or bayesian
metric:
  goal: maximize # 'maximize' or 'minimize'
  name: milp_eval_total_profit_best_model
run_cap: 20
parameters:
  use_wandb:
    value: True
  verbose:
    value: 0
  seed:
    value: 100
  hour_horizon:
    value: 36
  days_per_episode:
    value: 14
  directory:
    value: 'sweep'
  subdirectory:
    value: 'sweep_milp_05_01'
  da_reward_at_dec:
    value: False
  total_episodes:
    distribution: 'int_uniform'
    min: 7000   # 6000
    max: 12000  # 12000
  environment:
    parameters:
      battery_capacity:
        value: 20000
      battery_power:
        value: 10000
      battery_price:
        value: 3
      num_actions:
        values: [5, 7, 9]
      seed:
        value: 100
      standby_loss:
        value: 1.0 # for MILP comparison
      degradation:
        value: False # for MILP comparison
      calc_eff:
        value: False # for MILP comparison
      resolution:
        value: 'QH'
      train_data_name:
        value: 'DE_DA_QH_ID_train_adjusted_010_090.csv' # no '/' in front of the path
      test_data_name:
        value: 'DE_DA_QH_ID_test_adjusted_010_090.csv'  # no '/' in front of the path
  dqn_kwargs:
    parameters:
      learning_rate:
        distribution: 'log_uniform_values'
        min: 0.00001
        max: 0.1
      gamma:
        distribution: 'log_uniform_values'
        min: 0.999
        max: 0.99999
      tau:
        distribution: 'log_uniform_values'
        min: 0.05
        max: 0.3
      exploration_fraction:
        distribution: 'uniform'
        min: 0.1
        max: 0.5
      batch_size:
        values: [32, 64, 128]
      device:
        value: 'auto'  # just for GPU or CPU usage.
  eval_callback:
    value: True
  delta_ep_eval_call:
    value: 100
  ep_per_eval_call:
    value: 100
  ep_eval_policy:
    value: 1000
